root@idc705332:/ccrhx4.optimum-habana/examples/trl# DEEPSPEED_HPU_ZERO3_SYNC_MARK_STEP_REQUIRED=1 python ../gaudi_spawn.py --world_size 8 --use_deepspeed sft.py         --model_name_or_path "mistralai/Mixtral-8x7B-v0.1"         --deepspeed ../language-modeling/llama2_ds_zero3_config.json         --output_dir="./sft"         --max_steps=1000         --logging_steps=10         --per_device_train_batch_size=1         --per_device_eval_batch_size=1         --gradient_accumulation_steps=2         --learning_rate=1e-4         --lr_scheduler_type="cosine"         --warmup_steps=100         --weight_decay=0.05         --optim="paged_adamw_32bit"         --lora_target_modules "q_proj" "v_proj"         --bf16         --remove_unused_columns=False         --run_name="sft_mixtral"         --report_to=none         --use_habana         --use_lazy_mode           --dataset_name tatsu-lab/alpaca
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank --master_port 29500 sft.py --model_name_or_path mistralai/Mixtral-8x7B-v0.1 --deepspeed ../language-modeling/llama2_ds_zero3_config.json --output_dir=./sft --max_steps=1000 --logging_steps=10 --per_device_train_batch_size=1 --per_device_eval_batch_size=1 --gradient_accumulation_steps=2 --learning_rate=1e-4 --lr_scheduler_type=cosine --warmup_steps=100 --weight_decay=0.05 --optim=paged_adamw_32bit --lora_target_modules q_proj v_proj --bf16 --remove_unused_columns=False --run_name=sft_mixtral --report_to=none --use_habana --use_lazy_mode --dataset_name tatsu-lab/alpaca
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
[2024-04-30 02:08:28,910] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-04-30 02:08:30,428] [WARNING] [runner.py:206:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-30 02:08:30,499] [INFO] [runner.py:585:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None sft.py --model_name_or_path mistralai/Mixtral-8x7B-v0.1 --deepspeed ../language-modeling/llama2_ds_zero3_config.json --output_dir=./sft --max_steps=1000 --logging_steps=10 --per_device_train_batch_size=1 --per_device_eval_batch_size=1 --gradient_accumulation_steps=2 --learning_rate=1e-4 --lr_scheduler_type=cosine --warmup_steps=100 --weight_decay=0.05 --optim=paged_adamw_32bit --lora_target_modules q_proj v_proj --bf16 --remove_unused_columns=False --run_name=sft_mixtral --report_to=none --use_habana --use_lazy_mode --dataset_name tatsu-lab/alpaca
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
[2024-04-30 02:08:32,133] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-04-30 02:08:33,651] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-04-30 02:08:33,651] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-04-30 02:08:33,651] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-04-30 02:08:33,651] [INFO] [launch.py:163:main] dist_world_size=8
[2024-04-30 02:08:33,651] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
[2024-04-30 02:08:37,238] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
[2024-04-30 02:08:37,238] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
[2024-04-30 02:08:37,239] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
[2024-04-30 02:08:37,239] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
[2024-04-30 02:08:37,264] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
[2024-04-30 02:08:37,290] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
[2024-04-30 02:08:37,332] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.
  warnings.warn(
[2024-04-30 02:08:37,352] [INFO] [real_accelerator.py:178:get_accelerator] Setting ds_accelerator to hpu (auto detect)
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
[2024-04-30 02:08:40,077] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-04-30 02:08:40,077] [INFO] [comm.py:637:init_distributed] cdb=None
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-04-30 02:08:40,145] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-04-30 02:08:40,145] [INFO] [comm.py:637:init_distributed] cdb=None
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-04-30 02:08:40,418] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-04-30 02:08:40,418] [INFO] [comm.py:637:init_distributed] cdb=None
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-04-30 02:08:40,627] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-04-30 02:08:40,627] [INFO] [comm.py:637:init_distributed] cdb=None
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-04-30 02:08:41,012] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-04-30 02:08:41,012] [INFO] [comm.py:637:init_distributed] cdb=None
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-04-30 02:08:41,304] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-04-30 02:08:41,304] [INFO] [comm.py:637:init_distributed] cdb=None
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
[2024-04-30 02:08:41,501] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-04-30 02:08:41,501] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-30 02:08:41,501] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
[2024-04-30 02:08:41,507] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented
[2024-04-30 02:08:41,507] [INFO] [comm.py:637:init_distributed] cdb=None
/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: "hpu:X" notation is not supported by Gaudi PyTorch intergration bridge. Please change to "hpu" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)
  tensor: Tensor = fn(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: "hpu:X" notation is not supported by Gaudi PyTorch intergration bridge. Please change to "hpu" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)
  tensor: Tensor = fn(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: "hpu:X" notation is not supported by Gaudi PyTorch intergration bridge. Please change to "hpu" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)
  tensor: Tensor = fn(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: "hpu:X" notation is not supported by Gaudi PyTorch intergration bridge. Please change to "hpu" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)
  tensor: Tensor = fn(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: "hpu:X" notation is not supported by Gaudi PyTorch intergration bridge. Please change to "hpu" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)
  tensor: Tensor = fn(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: "hpu:X" notation is not supported by Gaudi PyTorch intergration bridge. Please change to "hpu" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)
  tensor: Tensor = fn(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/partition_parameters.py:238: UserWarning: "hpu:X" notation is not supported by Gaudi PyTorch intergration bridge. Please change to "hpu" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)
  tensor: Tensor = fn(*args, **kwargs)
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 0
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056375272 KB
------------------------------------------------------------------------------
[2024-04-30 02:08:49,327] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 995, num_elems = 46.70B
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.28s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.28s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.28s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.28s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.28s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.28s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.28s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:24<00:00,  1.29s/it]
tatsu-lab/alpaca data/finetune train
tatsu-lab/alpaca data/finetune train
tatsu-lab/alpaca data/finetune train
tatsu-lab/alpaca data/finetune train
tatsu-lab/alpaca data/finetune train
tatsu-lab/alpaca data/finetune train
tatsu-lab/alpaca data/finetune train
tatsu-lab/alpaca data/finetune train
Loading the dataset in streaming mode
  0%|                                                                                                                                                                      | 0/400 [00:00<?, ?it/s]Loading the dataset in streaming mode
  0%|                                                                                                                                                                      | 0/400 [00:00<?, ?it/s]Loading the dataset in streaming mode
  0%|                                                                                                                                                                      | 0/400 [00:00<?, ?it/s]Loading the dataset in streaming mode
  0%|                                                                                                                                                                      | 0/400 [00:00<?, ?it/s]Loading the dataset in streaming mode
  0%|                                                                                                                                                                      | 0/400 [00:00<?, ?it/s]Loading the dataset in streaming mode
  0%|                                                                                                                                                                      | 0/400 [00:00<?, ?it/s]Loading the dataset in streaming mode
  0%|                                                                                                                                                                      | 0/400 [00:00<?, ?it/s]Loading the dataset in streaming mode
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 488.97it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 509.16it/s]
The character to token ratio of the dataset is: 4.12
The character to token ratio of the dataset is: 4.12
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 501.86it/s]
The character to token ratio of the dataset is: 4.12
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 514.33it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 461.41it/s]
The character to token ratio of the dataset is: 4.12
The character to token ratio of the dataset is: 4.12
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 442.08it/s]
The character to token ratio of the dataset is: 4.12
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 430.03it/s]
The character to token ratio of the dataset is: 4.12
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 539.37it/s]
The character to token ratio of the dataset is: 4.12
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:156: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:156: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:238: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:238: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:156: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:238: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:156: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:156: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:238: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:238: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:156: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:156: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:238: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:238: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:156: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/ccrhx4.optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:238: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.
  warnings.warn(
Parameter Offload: Total persistent parameters: 4722688 in 225 params
{'loss': 9.5007, 'grad_norm': 90.9224853515625, 'learning_rate': 1e-05, 'epoch': 0.01, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}   
{'loss': 9.7178, 'grad_norm': 30.496784210205078, 'learning_rate': 2e-05, 'epoch': 0.02, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62} 
{'loss': 9.5845, 'grad_norm': 29930.9140625, 'learning_rate': 3e-05, 'epoch': 0.03, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}      
{'loss': 9.4162, 'grad_norm': 848.8640747070312, 'learning_rate': 4e-05, 'epoch': 0.04, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}  
{'loss': 9.1054, 'grad_norm': 3651.920654296875, 'learning_rate': 5e-05, 'epoch': 0.05, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}  
{'loss': 8.827, 'grad_norm': 2453.8603515625, 'learning_rate': 6e-05, 'epoch': 0.06, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}     
{'loss': 8.5195, 'grad_norm': 462.7321472167969, 'learning_rate': 7e-05, 'epoch': 0.07, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}  
{'loss': 7.9692, 'grad_norm': 94.00762176513672, 'learning_rate': 8e-05, 'epoch': 0.08, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}  
{'loss': 7.6381, 'grad_norm': 9476.8369140625, 'learning_rate': 9e-05, 'epoch': 0.09, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}    
{'loss': 7.4416, 'grad_norm': 58.72173309326172, 'learning_rate': 0.0001, 'epoch': 0.1, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}  
{'loss': 7.2569, 'grad_norm': 7.254589080810547, 'learning_rate': 9.99695413509548e-05, 'epoch': 0.11, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 7.1345, 'grad_norm': 57.04697799682617, 'learning_rate': 9.987820251299122e-05, 'epoch': 0.12, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 7.0916, 'grad_norm': 0.7540068626403809, 'learning_rate': 9.972609476841367e-05, 'epoch': 0.13, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 7.0357, 'grad_norm': 0.606721043586731, 'learning_rate': 9.951340343707852e-05, 'epoch': 0.14, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 7.036, 'grad_norm': 1.4238108396530151, 'learning_rate': 9.924038765061042e-05, 'epoch': 0.15, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.9981, 'grad_norm': 0.5956301689147949, 'learning_rate': 9.890738003669029e-05, 'epoch': 0.16, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.981, 'grad_norm': 0.6230586171150208, 'learning_rate': 9.851478631379982e-05, 'epoch': 0.17, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.9588, 'grad_norm': 0.8915256261825562, 'learning_rate': 9.806308479691595e-05, 'epoch': 0.18, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
 19%|████████████████████████████▊                                                                                                                            | 188/1000 [19:33<1:22:22,  6.09s/it]/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:431: UserWarning: The dataset reached end and the iterator is reset to the start.
  warnings.warn("The dataset reached end and the iterator is reset to the start.")
{'loss': 6.8745, 'grad_norm': 1.2565354108810425, 'learning_rate': 9.755282581475769e-05, 'epoch': 0.19, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.5957, 'grad_norm': 2.4128124713897705, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.2, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.0705, 'grad_norm': 4.274923801422119, 'learning_rate': 9.635919272833938e-05, 'epoch': 0.21, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 5.87, 'grad_norm': 2.7761454582214355, 'learning_rate': 9.567727288213005e-05, 'epoch': 0.22, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 5.5246, 'grad_norm': 3.7070724964141846, 'learning_rate': 9.493970231495835e-05, 'epoch': 0.23, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 5.488, 'grad_norm': 2372.100830078125, 'learning_rate': 9.414737964294636e-05, 'epoch': 0.24, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.418, 'grad_norm': 9466.408203125, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.25, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.9844, 'grad_norm': 32183.169921875, 'learning_rate': 9.24024048078213e-05, 'epoch': 0.26, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 7.1545, 'grad_norm': 19373.513671875, 'learning_rate': 9.145187862775209e-05, 'epoch': 0.27, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 7.0248, 'grad_norm': 19101.10546875, 'learning_rate': 9.045084971874738e-05, 'epoch': 0.28, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.7421, 'grad_norm': 6325.888671875, 'learning_rate': 8.940053768033609e-05, 'epoch': 0.29, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 6.162, 'grad_norm': 4530.244140625, 'learning_rate': 8.83022221559489e-05, 'epoch': 0.3, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 5.5813, 'grad_norm': 9.091714859008789, 'learning_rate': 8.715724127386972e-05, 'epoch': 0.31, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 5.2327, 'grad_norm': 4.341612815856934, 'learning_rate': 8.596699001693255e-05, 'epoch': 0.32, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 4.9682, 'grad_norm': 5.652708053588867, 'learning_rate': 8.473291852294987e-05, 'epoch': 0.33, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 4.8677, 'grad_norm': 28.130413055419922, 'learning_rate': 8.345653031794292e-05, 'epoch': 0.34, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 4.7703, 'grad_norm': 155.21217346191406, 'learning_rate': 8.213938048432697e-05, 'epoch': 0.35, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 4.4228, 'grad_norm': 3.372295379638672, 'learning_rate': 8.07830737662829e-05, 'epoch': 0.36, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 4.3848, 'grad_norm': 20.419523239135742, 'learning_rate': 7.938926261462366e-05, 'epoch': 0.37, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 4.365, 'grad_norm': 6.003672122955322, 'learning_rate': 7.795964517353735e-05, 'epoch': 0.38, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 4.238, 'grad_norm': 3.100510597229004, 'learning_rate': 7.649596321166024e-05, 'epoch': 0.39, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.9536, 'grad_norm': 2.5732028484344482, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.4, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.8897, 'grad_norm': 3.5844430923461914, 'learning_rate': 7.347357813929454e-05, 'epoch': 0.41, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.982, 'grad_norm': 5.061379432678223, 'learning_rate': 7.191855733945387e-05, 'epoch': 0.42, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.8227, 'grad_norm': 3.9395952224731445, 'learning_rate': 7.033683215379002e-05, 'epoch': 0.43, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.8408, 'grad_norm': 3.724548101425171, 'learning_rate': 6.873032967079561e-05, 'epoch': 0.44, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 4.0946, 'grad_norm': 2.3241806030273438, 'learning_rate': 6.710100716628344e-05, 'epoch': 0.45, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.8166, 'grad_norm': 9.059303283691406, 'learning_rate': 6.545084971874738e-05, 'epoch': 0.46, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.833, 'grad_norm': 3.063685417175293, 'learning_rate': 6.378186779084995e-05, 'epoch': 0.47, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.7243, 'grad_norm': 2.536508083343506, 'learning_rate': 6.209609477998338e-05, 'epoch': 0.48, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.7624, 'grad_norm': 67.9752426147461, 'learning_rate': 6.0395584540887963e-05, 'epoch': 0.49, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.5894, 'grad_norm': 2.322117567062378, 'learning_rate': 5.868240888334653e-05, 'epoch': 0.5, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
 50%|█████████████████████████████████████████████████████████████████████████████▌                                                                             | 500/1000 [52:04<52:41,  6.32s/it]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 3.5686, 'grad_norm': 3.002434015274048, 'learning_rate': 5.695865504800327e-05, 'epoch': 0.51, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.744, 'grad_norm': 2.4020612239837646, 'learning_rate': 5.522642316338268e-05, 'epoch': 0.52, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.6754, 'grad_norm': 2.8510260581970215, 'learning_rate': 5.348782368720626e-05, 'epoch': 0.53, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.5892, 'grad_norm': 3.627211809158325, 'learning_rate': 5.174497483512506e-05, 'epoch': 0.54, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.5616, 'grad_norm': 4.107828140258789, 'learning_rate': 5e-05, 'epoch': 0.55, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}  
{'loss': 3.5885, 'grad_norm': 4.372209072113037, 'learning_rate': 4.825502516487497e-05, 'epoch': 0.56, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.6022, 'grad_norm': 2.550837993621826, 'learning_rate': 4.6512176312793736e-05, 'epoch': 0.57, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.611, 'grad_norm': 2.6872265338897705, 'learning_rate': 4.477357683661734e-05, 'epoch': 0.58, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.5054, 'grad_norm': 12.162611961364746, 'learning_rate': 4.3041344951996746e-05, 'epoch': 0.59, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.4993, 'grad_norm': 4.138113021850586, 'learning_rate': 4.131759111665349e-05, 'epoch': 0.6, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.452, 'grad_norm': 2.2321925163269043, 'learning_rate': 3.960441545911204e-05, 'epoch': 0.61, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.475, 'grad_norm': 4.031575679779053, 'learning_rate': 3.790390522001662e-05, 'epoch': 0.62, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.5011, 'grad_norm': 12.263404846191406, 'learning_rate': 3.6218132209150045e-05, 'epoch': 0.63, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.376, 'grad_norm': 2.9671573638916016, 'learning_rate': 3.4549150281252636e-05, 'epoch': 0.64, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.4624, 'grad_norm': 2.295027256011963, 'learning_rate': 3.289899283371657e-05, 'epoch': 0.65, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.2826, 'grad_norm': 1.7962064743041992, 'learning_rate': 3.12696703292044e-05, 'epoch': 0.66, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3344, 'grad_norm': 1.9291160106658936, 'learning_rate': 2.9663167846209998e-05, 'epoch': 0.67, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.4806, 'grad_norm': 3.8009426593780518, 'learning_rate': 2.8081442660546125e-05, 'epoch': 0.68, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.456, 'grad_norm': 2.609137773513794, 'learning_rate': 2.6526421860705473e-05, 'epoch': 0.69, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 691/1000 [1:12:41<31:40,  6.15s/it]/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:431: UserWarning: The dataset reached end and the iterator is reset to the start.
  warnings.warn("The dataset reached end and the iterator is reset to the start.")
{'loss': 3.3405, 'grad_norm': 6.969494819641113, 'learning_rate': 2.500000000000001e-05, 'epoch': 0.7, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3611, 'grad_norm': 2.861863613128662, 'learning_rate': 2.350403678833976e-05, 'epoch': 0.71, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3799, 'grad_norm': 4.386013031005859, 'learning_rate': 2.2040354826462668e-05, 'epoch': 0.72, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3163, 'grad_norm': 3.170941114425659, 'learning_rate': 2.061073738537635e-05, 'epoch': 0.73, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3864, 'grad_norm': 2.2636096477508545, 'learning_rate': 1.9216926233717085e-05, 'epoch': 0.74, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3096, 'grad_norm': 1.983092188835144, 'learning_rate': 1.7860619515673033e-05, 'epoch': 0.75, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.2846, 'grad_norm': 7.7007293701171875, 'learning_rate': 1.6543469682057106e-05, 'epoch': 0.76, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.4222, 'grad_norm': 2.0669126510620117, 'learning_rate': 1.526708147705013e-05, 'epoch': 0.77, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3371, 'grad_norm': 27.854581832885742, 'learning_rate': 1.4033009983067452e-05, 'epoch': 0.78, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.4539, 'grad_norm': 912.274658203125, 'learning_rate': 1.2842758726130283e-05, 'epoch': 0.79, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3099, 'grad_norm': 2.5080881118774414, 'learning_rate': 1.1697777844051105e-05, 'epoch': 0.8, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3612, 'grad_norm': 15864.7353515625, 'learning_rate': 1.0599462319663905e-05, 'epoch': 0.81, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.4466, 'grad_norm': 189.39320373535156, 'learning_rate': 9.549150281252633e-06, 'epoch': 0.82, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3852, 'grad_norm': 8877.490234375, 'learning_rate': 8.548121372247918e-06, 'epoch': 0.83, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3555, 'grad_norm': 116.07120513916016, 'learning_rate': 7.597595192178702e-06, 'epoch': 0.84, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.439, 'grad_norm': 3324.896484375, 'learning_rate': 6.698729810778065e-06, 'epoch': 0.85, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3562, 'grad_norm': 146561.15625, 'learning_rate': 5.852620357053651e-06, 'epoch': 0.86, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.2777, 'grad_norm': 2.0090057849884033, 'learning_rate': 5.060297685041659e-06, 'epoch': 0.87, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3513, 'grad_norm': 54.38397216796875, 'learning_rate': 4.322727117869951e-06, 'epoch': 0.88, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.2754, 'grad_norm': 4.059368133544922, 'learning_rate': 3.6408072716606346e-06, 'epoch': 0.89, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.2976, 'grad_norm': 104.37793731689453, 'learning_rate': 3.0153689607045845e-06, 'epoch': 0.9, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3216, 'grad_norm': 2.157804250717163, 'learning_rate': 2.4471741852423237e-06, 'epoch': 0.91, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3013, 'grad_norm': 2.0507166385650635, 'learning_rate': 1.9369152030840556e-06, 'epoch': 0.92, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3135, 'grad_norm': 5.603562831878662, 'learning_rate': 1.4852136862001764e-06, 'epoch': 0.93, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3874, 'grad_norm': 4.210592746734619, 'learning_rate': 1.0926199633097157e-06, 'epoch': 0.94, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.294, 'grad_norm': 2.5601885318756104, 'learning_rate': 7.596123493895991e-07, 'epoch': 0.95, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3097, 'grad_norm': 3.619208812713623, 'learning_rate': 4.865965629214819e-07, 'epoch': 0.96, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3247, 'grad_norm': 1.911553978919983, 'learning_rate': 2.7390523158633554e-07, 'epoch': 0.97, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.2765, 'grad_norm': 1.9319934844970703, 'learning_rate': 1.2179748700879012e-07, 'epoch': 0.98, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.3851, 'grad_norm': 1.9971814155578613, 'learning_rate': 3.04586490452119e-08, 'epoch': 0.99, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
{'loss': 3.2639, 'grad_norm': 2.1190221309661865, 'learning_rate': 0.0, 'epoch': 1.0, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}    
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:44:40<00:00,  6.22s/it]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'train_runtime': 6315.1629, 'train_samples_per_second': 2.534, 'train_steps_per_second': 0.158, 'train_loss': 4.76651997756958, 'epoch': 1.0, 'memory_allocated (GB)': 11.64, 'max_memory_allocated (GB)': 48.68, 'total_memory_available (GB)': 94.62}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:45:15<00:00,  6.32s/it]
^CReceived Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
[2024-04-30 04:15:55,242] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 9520
Traceback (most recent call last):
  File "/ccrhx4.optimum-habana/examples/trl/../gaudi_spawn.py", line 110, in <module>
    main()
  File "/ccrhx4.optimum-habana/examples/trl/../gaudi_spawn.py", line 105, in main
    ret_code = distributed_runner.run()
  File "/ccrhx4.optimum-habana/optimum/habana/distributed/distributed_runner.py", line 218, in run
    proc.wait()
  File "/usr/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/usr/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/usr/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

root@idc705332:/ccrhx4.optimum-habana/examples/trl# [2024-04-30 04:16:04,053] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 9521

